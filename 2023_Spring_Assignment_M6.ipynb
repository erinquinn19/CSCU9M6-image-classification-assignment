{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erinquinn19/CSCU9M6-image-classification-assignment/blob/main/2023_Spring_Assignment_M6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#University of Stirling - Spring 2023\n",
        "\n",
        "## CSCU9M6 - Natural Language Processing and Computer Vision (2022/3)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4i5afvUbhmGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Summary\n",
        "\n",
        "In this activity, you are required to apply the knowledge acquired in this module through the design and development of a complete project for image classification in an application to be defined by yourself. For this, you will need to perform the following **mandatory** steps:\n",
        "\n",
        "1. [Problem definition](#scrollTo=hglJVRRslqMn)\n",
        "2. [GitHub repository](#scrollTo=ecxDhkV9qmUf)\n",
        "3. [Dataset](#scrollTo=qEgFzxmWrGA9)\n",
        "4. [Dataloader](#scrollTo=EDd6lLwlx4un)\n",
        "5. [Proposed solution](#scrollTo=ScTrpUW8zOp4)\n",
        "6. [Experimental tests and evaluations](#scrollTo=3RBW58of0ZDo)\n",
        "7. [Quiz and Report](#scrollTo=ws14iV4Dp_vf)\n",
        "\n",
        "**Deadlines** and other details can be seen on Canvas [\\[link\\]](https://canvas.stir.ac.uk/courses/12587/assignments/102373)."
      ],
      "metadata": {
        "id": "cJZF464wk6Xh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 1. **Problem definition** \n",
        "\n",
        "\n",
        "In this assignment, you are required to apply the knowledge acquired in the module to solve a classification problem from images collected in the context of two different cities (A and B).\n",
        " - If the work is being carried out in pairs, **cities A and B must be the hometowns of each student**. In the case of individual work, city A must be your hometown and city B must be Stirling (or Edinburgh, if needed).\n",
        " - The standard recommendation is that the project focuses on classifying cars or trees image scenes, which are easier to identify and annotate. Other objects or phenomena can be adopted, but are subject to prior approval by the module instructor (Jefersson A. dos Santos). **You are not allowed to assemble datasets containing people. Other sensitive patterns, such as license plates, must be properly hidden.**\n",
        " - Don't panic! We are aware that acquiring images _in situ_ is an impediment for most students. The dataset can be assembled with images collected remotely or from public repositories. Just be careful with rights and permissions for using images found on the internet. Anyway, these factors must be taken into account for the problem definition.\n",
        " - While we encourage you to do interesting and engaging work, it shouldn't be too complex or time-consuming. Try to appropriately scale the time required for this step. Ask the instructors for advice, if necessary. **GA students:** you are encouraged to link the project with your work activities, but keep in mind you still need to construct two datasets (A and B). \n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)\n",
        " "
      ],
      "metadata": {
        "id": "hglJVRRslqMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 2. **GitHub repository**\n",
        "\n",
        "Give your project a name, create a private [GitHub repository](https://github.com/) with the name [Module Code] + [Project Name] and give access to the module instructors. Create a cover page with a description of your project. This empty notebook must be uploaded in the repository as well as the created dataset. The deadline to perform this task will be 10 days after the publication of this notebook. \n",
        "This notebook should be updated and committed to the repository according to the deadlines.\n",
        "The repository's update history will be used as a criterion for monitoring and evaluating the work.\n",
        "**Check the videos provided in the extra section on Canvas for more details on how to create your GitHub repository** [\\[link\\]](https://canvas.stir.ac.uk/courses/12587/pages/extra-session-cnn-hyperparameters-and-github).\n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "ecxDhkV9qmUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 3. **Dataset creation**\n",
        "\n",
        "You must collect a minimum of **200 positive samples** from the study objects for each city (A and B). \n",
        "Note that, depending on the task being solved, it will also be necessary to collect more samples - negative ones, for instance.\n",
        "\n",
        "Your dataset can be assembled from one or more of the following ways:\n",
        "\n",
        "  - *M1* - Pictures taken by yourself on site (street view from cities A and B), with attention to anonymization issues (if it is the case). It is not allowed to assemble datasets containing people. Other sensitive patterns, such as license plates, must be properly hidden.\n",
        "\n",
        "  - *M2* - Aerial satellite/drone images obtained from GIS and remote sensing platforms or public repositories. Be careful with unusual file formats that may be challenging to manipulate using basic image processing libraries. We recommend keeping or converting the images to jpg or png.\n",
        "\n",
        "  - *M3* - Pictures taken from other public available datasets. Remember you are not allowed to use datasets containing people or other sensitive patterns/objects.\n",
        "\n",
        "  - *M4* - Images crawled from the internet as a whole (social networks, webpages, etc), with special attention to use and copyrights.\n",
        "\n",
        "  - *M5* - Textual and metadata you may need in your project, with special attention to use and copyrights (as always!).\n",
        "\n",
        "**Important:** If you collect the images on your own or from aerial imagery repositories, it will be necessary to keep the geographic coordinates. If you collect from specific websites, please retain the source links. This information should be placed in a .csv file and made available along with the final dataset.\n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "qEgFzxmWrGA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 4. **Dataloader**\n",
        "\n",
        "Here you are required to implement all the code related to pre-processing, cleaning, de-noising and preparing the input images and metadata according to the necessary data structures as input to your pattern recognition module. We recommend using [PyTorch](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) or [Tensorflow (with Keras)](https://keras.io/getting_started/intro_to_keras_for_engineers/) as a base, but you are free to use any library or platform as long as it is well justified in the [final report](#scrollTo=ws14iV4Dp_vf).\n",
        "\n",
        "[top](scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "EDd6lLwlx4un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your dataloader code here. Create more code cells if you find it necessary\n",
        "!wget https://www.dropbox.com/s/9yw3sjht6718tz6/dataset.zip?dl=00\n",
        "!unzip dataset.zip?dl=00\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"dataset%20%282%29.zip?dl=0\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"dataset%20%282%29.zip?dl=0\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n"
      ],
      "metadata": {
        "id": "RaPd82NmyNCK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f0d38a-292d-4498-fee1-9eba844f1539"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-05 11:49:01--  https://www.dropbox.com/s/9yw3sjht6718tz6/dataset.zip?dl=00\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/9yw3sjht6718tz6/dataset.zip [following]\n",
            "--2023-04-05 11:49:01--  https://www.dropbox.com/s/raw/9yw3sjht6718tz6/dataset.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucbc45073fdd2a8dbd9832408a4a.dl.dropboxusercontent.com/cd/0/inline/B5mK0rDFCkZU-QGVAnYTiIIB_7ymmVgLx6y8qr8B5BpQ5tBxsXp1YTzh4DFWeblA9UltyOXlJHG2o2wy_zddPkomm8zaSvw6ekj5aPKQbaZhCIIG8FtJR-uzd8mWasznDRLNumzDLGKUhdoudfeTmGjpOC2XXBNCC5OczPHID-XcSg/file# [following]\n",
            "--2023-04-05 11:49:05--  https://ucbc45073fdd2a8dbd9832408a4a.dl.dropboxusercontent.com/cd/0/inline/B5mK0rDFCkZU-QGVAnYTiIIB_7ymmVgLx6y8qr8B5BpQ5tBxsXp1YTzh4DFWeblA9UltyOXlJHG2o2wy_zddPkomm8zaSvw6ekj5aPKQbaZhCIIG8FtJR-uzd8mWasznDRLNumzDLGKUhdoudfeTmGjpOC2XXBNCC5OczPHID-XcSg/file\n",
            "Resolving ucbc45073fdd2a8dbd9832408a4a.dl.dropboxusercontent.com (ucbc45073fdd2a8dbd9832408a4a.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to ucbc45073fdd2a8dbd9832408a4a.dl.dropboxusercontent.com (ucbc45073fdd2a8dbd9832408a4a.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/B5nIEEVV2NrsfnAs4eMFRQuTwvlKLt3zBPyp55qoo4s5ARN7NJrHvmiVE3A26KPda01ESZq_FVY4lnLWYhNdRTejUjT_EkB0Eh_6BI0xVZxJR6bUv_R7UOSp5yvkEN6A7YSs1ag1PSlyf9Qs15MwwBc0db_5_88Z6wP6HC5vya0EgH4LFrrykNO6HpVlTgkj85Z2C4N31wDGmLuFPMTeJ9C5-tblief5iRzFWdpKRXFoW_SnsNr_2CNbCclDHJvXky-eonEtP-XRaiWCE8FzOxsd8jUCTJPmvUBIG8mIFciBnc04uuR1PFduZD0cXPqzoI4nu2kAz8XNB4bh3NrYJkzEmKWSL7DNwxHpco5oNVPfIkscDpuhHFDebm8wJehEqVkIFU8YK-gXQ_6ofBWLjiwhfurIlImaLyjczvIZ3JF5aw/file [following]\n",
            "--2023-04-05 11:49:06--  https://ucbc45073fdd2a8dbd9832408a4a.dl.dropboxusercontent.com/cd/0/inline2/B5nIEEVV2NrsfnAs4eMFRQuTwvlKLt3zBPyp55qoo4s5ARN7NJrHvmiVE3A26KPda01ESZq_FVY4lnLWYhNdRTejUjT_EkB0Eh_6BI0xVZxJR6bUv_R7UOSp5yvkEN6A7YSs1ag1PSlyf9Qs15MwwBc0db_5_88Z6wP6HC5vya0EgH4LFrrykNO6HpVlTgkj85Z2C4N31wDGmLuFPMTeJ9C5-tblief5iRzFWdpKRXFoW_SnsNr_2CNbCclDHJvXky-eonEtP-XRaiWCE8FzOxsd8jUCTJPmvUBIG8mIFciBnc04uuR1PFduZD0cXPqzoI4nu2kAz8XNB4bh3NrYJkzEmKWSL7DNwxHpco5oNVPfIkscDpuhHFDebm8wJehEqVkIFU8YK-gXQ_6ofBWLjiwhfurIlImaLyjczvIZ3JF5aw/file\n",
            "Reusing existing connection to ucbc45073fdd2a8dbd9832408a4a.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 55824212 (53M) [application/zip]\n",
            "Saving to: ‘dataset.zip?dl=00’\n",
            "\n",
            "dataset.zip?dl=00   100%[===================>]  53.24M  46.9MB/s    in 1.1s    \n",
            "\n",
            "2023-04-05 11:49:08 (46.9 MB/s) - ‘dataset.zip?dl=00’ saved [55824212/55824212]\n",
            "\n",
            "Archive:  dataset.zip?dl=00\n",
            "  inflating: dataset/desktop.ini     \n",
            "   creating: dataset/Positives/\n",
            "  inflating: dataset/Positives/glasgow1_jpg_25.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_26.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_28.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_29.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_30.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_32.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_33.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_36.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_37.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_38.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_40.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_41.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_46.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_47.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_48.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_49.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_50.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_51.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_53.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_57.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_58 (1).jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_58.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_60.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_61.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_70.jpg  \n",
            "  inflating: dataset/Positives/glasgow1_jpg_71.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_24.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_30.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_31.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_32.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_33.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_36.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_37.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_38.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_44.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_45.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_48.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_51.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_52.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_57.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_60.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_62.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_63.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_64.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_65.jpg  \n",
            "  inflating: dataset/Positives/glasgow2_jpg_66.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_25.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_26.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_27.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_28.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_29.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_30.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_31.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_32.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_33.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_34.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_36.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_37.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_38.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_39.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_40.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_41.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_42.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_43.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_44.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_45.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_46.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_48.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_49.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_50.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_53.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_54.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_55.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_56.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_60.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_61.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_64.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_65.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_66.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_67.jpg  \n",
            "  inflating: dataset/Positives/glasgow3_jpg_70.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_24.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_25.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_26.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_28.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_29.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_30.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_31 (1).jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_32.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_33.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_34.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_36.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_37.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_38.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_39.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_41.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_42.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_43.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_46.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_47.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_48.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_49.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_51.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_52.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_55.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_57.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_58.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_59.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_66.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_68.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_69.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_70.jpg  \n",
            "  inflating: dataset/Positives/glasgow4_jpg_71.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_24.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_25.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_26.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_27.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_28.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_29.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_30.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_37.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_38.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_40.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_41.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_42.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_43.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_50.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_51.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_52.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_54.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_55.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_57.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_58.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_60.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_62.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_63.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_66.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_67.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_68.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_69.jpg  \n",
            "  inflating: dataset/Positives/glasgow5_jpg_70.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_24.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_25.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_26.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_27.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_28.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_29.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_30.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_31.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_32.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_33.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_34.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_36.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_37.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_38.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_39.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_40.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_41.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_42.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_43.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_44.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_45.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_46.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_49.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_50.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_51.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_52.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_53.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_55.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_56.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_57.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_58.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_64.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_65.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_66.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_67.jpg  \n",
            "  inflating: dataset/Positives/glasgow6_jpg_68.jpg  \n",
            "   creating: dataset/Positives/negatives/\n",
            "  inflating: dataset/Positives/negatives/glasgow1_jpg_45.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow2_jpg_25.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow2_jpg_26.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow2_jpg_28.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow2_jpg_35.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow2_jpg_39.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow2_jpg_40.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow2_jpg_43.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow2_jpg_49.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow2_jpg_50.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow2_jpg_53.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow2_jpg_55.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow2_jpg_70.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow3_jpg_24.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow3_jpg_35.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow3_jpg_47.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow3_jpg_57.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow3_jpg_58.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow3_jpg_59.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow3_jpg_62.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow3_jpg_63.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow3_jpg_68.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow3_jpg_69.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow4_jpg_35.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow4_jpg_40.jpg  \n",
            "  inflating: dataset/Positives/negatives/glasgow5_jpg_49.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_24.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_25.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_26.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_27.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_29.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_30.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_31.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_32.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_33.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_34.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_35.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_36.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_37.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_38.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_41.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_42.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_43.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_44.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_45.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_47.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_48.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_51.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_53.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_56.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_57.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_58.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_61.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_62.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_66.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_67.jpg  \n",
            "  inflating: dataset/Positives/stirling1_jpg_69.jpg  \n",
            "  inflating: dataset/Positives/stirling2_jpg_24.jpg  \n",
            "  inflating: dataset/Positives/stirling2_jpg_25.jpg  \n",
            "  inflating: dataset/Positives/stirling2_jpg_26.jpg  \n",
            "  inflating: dataset/Positives/stirling2_jpg_27.jpg  \n",
            "  inflating: dataset/Positives/stirling2_jpg_28.jpg  \n",
            "  inflating: dataset/Positives/stirling2_jpg_30.jpg  \n",
            "  inflating: dataset/Positives/stirling2_jpg_31.jpg  \n",
            "  inflating: dataset/Positives/stirling2_jpg_32.jpg  \n",
            "  inflating: dataset/Positives/stirling2_jpg_33.jpg  \n",
            "  inflating: dataset/Positives/stirling2_jpg_36.jpg  \n",
            "  inflating: dataset/Positives/stirling2_jpg_37.jpg  \n",
            "  inflating: dataset/Positives/stirling2_jpg_38.jpg  \n",
            "  inflating: dataset/Positives/stirling2_jpg_44.jpg  \n",
            "  inflating: dataset/Positives/stirling2_jpg_45.jpg  \n",
            "  inflating: dataset/Positives/stirling2_jpg_46.jpg  \n",
            "  inflating: dataset/Positives/stirling2_jpg_57.jpg  \n",
            "  inflating: dataset/Positives/stirling2_jpg_58.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_24.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_25.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_30.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_32.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_37.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_38.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_41.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_42.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_43.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_44.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_45.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_50.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_51.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_54.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_55.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_56.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_57.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_58.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_61.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_65.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_66.jpg  \n",
            "  inflating: dataset/Positives/stirling3_jpg_67.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_24.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_25.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_26.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_27.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_28.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_29.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_30.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_31.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_32.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_33.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_34.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_35.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_36.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_37.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_38.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_39.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_40.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_41.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_42.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_43.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_44.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_45.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_46.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_47.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_48.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_49.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_50.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_51.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_52.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_53.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_54.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_55.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_56.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_57.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_58.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_59.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_60.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_61.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_62.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_63.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_64.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_65.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_66.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_67.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_68.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_69.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_70.jpg  \n",
            "  inflating: dataset/Positives/stirling4_jpg_71.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_27.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_28.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_29.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_30.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_31.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_32.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_33.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_34.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_39.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_41.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_42.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_43.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_45.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_53.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_55.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_56.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_57.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_58.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_60.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_61.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_62.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_63.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_64.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_65.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_66.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_67.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_68.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_69.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_70.jpg  \n",
            "  inflating: dataset/Positives/stirling5_jpg_71.jpg  \n",
            "  inflating: dataset/Positives/stirling6_jpg_25.jpg  \n",
            "  inflating: dataset/Positives/stirling6_jpg_26.jpg  \n",
            "  inflating: dataset/Positives/stirling6_jpg_29.jpg  \n",
            "  inflating: dataset/Positives/stirling6_jpg_30.jpg  \n",
            "  inflating: dataset/Positives/stirling6_jpg_38.jpg  \n",
            "  inflating: dataset/Positives/stirling6_jpg_39.jpg  \n",
            "  inflating: dataset/Positives/stirling6_jpg_41.jpg  \n",
            "  inflating: dataset/Positives/stirling6_jpg_42.jpg  \n",
            "  inflating: dataset/Positives/stirling6_jpg_43.jpg  \n",
            "  inflating: dataset/Positives/stirling6_jpg_53.jpg  \n",
            "  inflating: dataset/Positives/stirling6_jpg_54.jpg  \n",
            "  inflating: dataset/Positives/stirling6_jpg_55.jpg  \n",
            "  inflating: dataset/Positives/stirling6_jpg_65.jpg  \n",
            "  inflating: dataset/Positives/stirling6_jpg_66.jpg  \n",
            "  inflating: dataset/Positives/stirling6_jpg_67.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_25.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_26.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_27.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_28.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_29.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_30.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_31.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_32.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_33.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_36.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_37.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_38.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_39.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_40.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_41.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_42.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_43.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_46.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_48.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_49.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_50.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_51.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_52.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_53.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_57.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_58.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_60.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_61.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_62.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_63.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_64.jpg  \n",
            "  inflating: dataset/Positives/stirling7_jpg_69.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_24.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_25.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_26.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_27.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_28.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_29.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_30.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_31.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_32.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_33.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_34.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_35.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_37.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_38.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_39.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_40.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_41.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_42.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_43.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_44.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_45.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_46.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_47.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_48.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_49.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_50.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_51.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_52.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_53.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_54.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_55.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_56.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_57.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_58.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_59.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_61.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_62.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_63.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_64.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_65.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_66.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_67.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_68.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_69.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_70.jpg  \n",
            "  inflating: dataset/Positives/stirling8_jpg_71.jpg  \n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to dataset%20%282%29.zip?dl=0/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 15006518.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset%20%282%29.zip?dl=0/FashionMNIST/raw/train-images-idx3-ubyte.gz to dataset%20%282%29.zip?dl=0/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to dataset%20%282%29.zip?dl=0/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 263554.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset%20%282%29.zip?dl=0/FashionMNIST/raw/train-labels-idx1-ubyte.gz to dataset%20%282%29.zip?dl=0/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to dataset%20%282%29.zip?dl=0/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 4976143.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset%20%282%29.zip?dl=0/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to dataset%20%282%29.zip?dl=0/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to dataset%20%282%29.zip?dl=0/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 18083984.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset%20%282%29.zip?dl=0/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to dataset%20%282%29.zip?dl=0/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "# This package has several routines for image manipulation. This can be useful for implementing data augmentation as well.\n",
        "from torchvision.io import read_image \n",
        "\n",
        "# Here we will implement our \"MyTree Dataset\" class that extends the torch.utils.data.Dataset\n",
        "# This will make our dataset easily accessible for the necessary tensor formats to run on PyTorch.\n",
        "# More about tensors here: https://pytorch.org/docs/stable/tensors.html\n",
        "class MyTreeDataset(Dataset):\n",
        "\n",
        "    # The __init__ function is run once when instantiating the Dataset object. \n",
        "    #We initialize the directory containing the images, the annotations file, and both transforms.\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None, resize=(28, 28)):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    # The __len__ function returns the number of samples in our dataset.\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    # The __getitem__ function loads and returns a sample from the dataset at the given index idx. \n",
        "    # Based on the index, it identifies the image’s location on disk, converts that to a tensor using read_image, \n",
        "    # retrieves the corresponding label from the csv data in self.img_labels, calls the transform functions \n",
        "    # on them (if applicable), and returns the tensor image and corresponding label in a tuple.\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Creating the main directory path. \n",
        "        dir_path = os.path.join(self.img_dir, str(self.img_labels.iloc[idx, 1]))\n",
        "        # As our dataset was organized separating each class by folder, we need to concatenate the name of the folders '0' and '1' to the name of the images.\n",
        "        img_path = os.path.join(dir_path, self.img_labels.iloc[idx, 0])\n",
        "        print(img_path)\n",
        "\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "\n",
        "        # This code snippet is important to receive the image as in tensor format.\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "tQp04ZXT7XSD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 5. **Proposed solution** \n",
        "\n",
        "This is where you should implement most of your code for your solution. Write the routines for training and predicting the models and any necessary intermediate steps. Post-processing functions must also be implemented here.\n",
        "\n",
        "  - Use good programming practices, modularizing and adequately commenting on your code. Code quality will be considered in the final assessment.\n",
        "\n",
        "  - You can use pre-trained models as backbones or any code available on the web as a basis, but they must be correctly credited and referenced both in this notebook and in the final report. Cite the source link repository and explicitly cite the authors of it.\n",
        "If you changed existing code, make it clear what the changes were.\n",
        "Make it clear where your own code starts and where it ends. Note that the originality percentage of the code will be considered in the evaluation, so use external codes wisely and sparingly. **Missconduct alert:** remember that there are many tools that compare existing source code and that it is relatively easy to identify authorship. So, be careful and fair by always properly thanking the authors if you use external code.\n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "ScTrpUW8zOp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your proposed solution code here. Create more code cells if you find it necessary\n",
        "\n",
        "!pip3 install torch torchvision\n"
      ],
      "metadata": {
        "id": "jJs8HpW_zX0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3196f90d-6cbf-4fa0-e8d9-bcf12e31f3af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time, os, sys, numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# PIL is an image package similar to OpenCV that we saw in the practical classes\n",
        "# https://pillow.readthedocs.io/en/stable/handbook/overview.html\n",
        "import PIL \n",
        "from PIL import Image\n",
        "from torch import optim\n",
        "from torchsummary import summary\n",
        "\n",
        "# Test if GPU is avaliable, if not, use cpu instead\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "n = torch.cuda.device_count()\n",
        "devices_ids= list(range(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA7QsYcTRxa_",
        "outputId": "98da6f22-28dd-4aa3-d92f-615d8fe93902"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IFHa9VmlR5AM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 6. **Experimental tests and evaluations** \n",
        "\n",
        "\n",
        "Here you must implement your code for training, testing and evaluating your solution. For this, the following code blocks (*E1*, *E2*, and *E3*) are mandatory:\n",
        "\n",
        "  - *E1* - Training the models. Implement code to call the dataloaders implemented for training your models.  Make routines to test different parameters of your models. Plot graphs that illustrate how parameters impact model training. Compare. Train and select a model for each city (A and B) and justify. You should use half (50%) of the samples from each dataset for training and leave the other half for testing (50%). \n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "3RBW58of0ZDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your codes for E1 here. Create more code cells if needed\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jHWwdXg32BEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - *E2* - Testing the models in the dataset. You must implement code routines to test the predictive ability of your models using half of each dataset intended for testing. **The model trained in city A must be tested in city A. The model trained in city B must be tested in city B.** Use the evaluation metrics (accuracy, F1-score, AUC, etc) that are most appropriate for your problem. Plot graphs that illustrate the results obtained for each city (A and B). Plot visual examples of correctly (true positive) and incorrectly (false positive) classified samples. \n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)\n"
      ],
      "metadata": {
        "id": "TunTimEv1szf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your codes for E2 here. Create more code cells if needed\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_s6ygCpi2CO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - *E3* - Testing the models crossing datasets. Here you must do exactly the same as in *E2*, but now training in one city and testing in the other. **The model trained in city A must be tested in city B. The model trained in city B must be tested in city A.** Use the same metrics and plot the same types of graphs so that results are comparable.\n",
        "\n",
        "[top](scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "bZ0zVXjQ1x0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your codes for E3 here. Create more code cells if needed\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cotguzxyo3Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 7. **Quiz and Report**\n",
        "\n",
        "Answer the assessment quiz that will be made available on Canvas one week before the final deadline. Make a 2-page report using the [IEEE template](https://www.overleaf.com/read/rdqwshtvyjdn) with a maximum of 1000 words. Latex is recommended, but you can deliver the report in MS Word if you prefer. Your report should contain five sections: introduction, description of the proposed solution with justifications, results (here you can include the same graphs and pictures generated in this jupyter notebook), discussion of the results, and conclusion. Properly cite references to articles, tutorials, and sources used. A pdf version of your report should be made available in the project's github repository under the name \"[project name] + _final_report.pdf\".\n",
        "\n",
        "\n",
        "[top](#scrollTo=4i5afvUbhmGo)"
      ],
      "metadata": {
        "id": "ws14iV4Dp_vf"
      }
    }
  ]
}